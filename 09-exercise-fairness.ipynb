{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 09: Fairness Assessment and Mitgation\n",
    "\n",
    "Welcome to the eighth exercise for Big Data Management.\n",
    "\n",
    "Your objectives for this session are to:\n",
    "\n",
    "- implement a `RandomForestClassifier` to predict recidivism with the COMPAS dataset,\n",
    "- inspect model performance and feature importance,\n",
    "- perform a fairness assessment, and\n",
    "- use `ThresholdOptimizer` for post-processing unfairness mitigation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's import the libraries for today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform a fairness assessment and mitigate unfairness, we'll use a library called `fairlearn`. Check out the `fairlearn` documentation [here](https://fairlearn.org/v0.9/quickstart.html).\n",
    "\n",
    "\n",
    "If you haven't installed `fairlearn` in the past, uncomment the code in the next block and run the installation command before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install fairlearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once `fairlearn` is installed, use the code below to import the necessary functions from the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fairness assessments\n",
    "from fairlearn.metrics import MetricFrame, true_positive_rate, false_positive_rate, selection_rate, equalized_odds_difference, demographic_parity_difference, selection_rate\n",
    "\n",
    "# for unfairness mitigation\n",
    "from fairlearn.postprocessing import ThresholdOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's read in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('propublica_data_for_fairml.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we'll be working with a preprocessed version of the COMPAS (which stands for Correctional Offender Management Profiling for Alternative Sanctions) data. In this dataset, the target variable is `Two_yr_Recidivism`: a binary class label indicating if a defendant actually re-offended within two years of their parole (1 if `yes` and 0 if `no`). As attributes, we have the number of prior offenses (`Number_of_Priors`) and various binary variables on the defendant's demographics and the nature of their past offence(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check out the original data [here](https://github.com/propublica/compas-analysis) and an article outlining ProPublica's analysis [here](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "### Part 1: Implementing a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning algorithms are increasingly adopted by judicial systems for predictive policing and recidivism prediction. In the case of Northepointe's COMPAS tool, the intention was to help U.S. judges decide whether to release defendants on parole. If a defendant is likely to re-offend, then they should not be released, and vice versa. COMPAS is a proprietary tool, which means exact details of its develop and functionality (e.g., what specific algorithm was used) are not publicly disclosed. \n",
    "\n",
    "Let's implement our own classifier. Use the code below to define your feature matrix `X` and target variable `y`, and then make a train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the feature matrix X and target vector y\n",
    "X = df.drop(\"Two_yr_Recidivism\", axis=1)\n",
    "y = df[\"Two_yr_Recidivism\"]\n",
    "\n",
    "# train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's tune a `RandomForestClassifier` with `GridSearchCV`. The code in the next block provides the hyperparameter grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the hyperparameters and possible values to check\n",
    "param_grid = {\n",
    "    'n_estimators': [200],\n",
    "    'max_depth': [5, 10, 15, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>TASK 1</font>\n",
    "\n",
    "Implement `GridSearchCV` to search through `param_grid` and find the best possible hyperparameter settings for a `RandomForestClassifier`. Save the tuned model as `best_rf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here - create the grid search object with the following arguments: cv=5, n_jobs=-1, verbose=3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here - fit the grid search object to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here - save the best estimator as `best_rf`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>TASK 2</font>\n",
    "\n",
    "Now you've got your tuned classifier defined as `best_rf`. Use it to generate predicted target values for your test data. Define your predicted target values as `y_pred`, and then print a `classification_report` and inspect your classifier's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here - predict the outcomes using the best RandomForest model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# your code here - generate and display the classification report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How's your classifier doing? Discuss the classification report with a classmate.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>TASK 3</font>\n",
    "\n",
    "In addition to performance metrics like those in the `classification_report`, it can be useful to inspect a model's feature importances. Feature importances tell us which features influence a model's predictions the most, increasing model transparency.\n",
    "\n",
    "Feature importance can be calculated in many different ways. With linear and logistic regression, for example, regression coefficients can be interpreted as feature importance. But with more complex models like our `RandomForesstClassifier`, there are no coefficients and we can't  visualize the model as a single tree, since it's an ensemble of many, many trees. \n",
    "\n",
    "Let's try two different ways of assessing feature importance with our `RandomForesstClassifier`: impurity-based feature importance and permutation feature importance.\n",
    "\n",
    "Impurity-based feature importance, also sometimes called \"Gini importance\" or \"mean decrease in impurity (MDI)\", measures how much a feature contributes to reducing the criterion used by a model. In other words, impurity-based feature importance tells us how much segmenting on a feature decreases impurity on average. A high value means the feature is important for predicting `y` in the dataset. Impurity-based feature importance is calculated for `RandomForesstClassifier` by default.\n",
    "\n",
    "Permutation feature importance measures the increase in the prediction error of the model after we permuted (shuffles) the featureâ€™s values, which breaks the relationship between the feature and the true outcome. Permutation feature importance can be calculated for any kind of model. A high value means the feature is important for accurately predicting `y` with the model. A negative value suggest that a feature is irrelvant, noisy, or otherwise unhelpful for pedicting `y`. Read more about it [here](https://christophm.github.io/interpretable-ml-book/feature-importance.html#example-and-interpretation), and in the scikit-learn documentation [here](https://scikit-learn.org/1.5/modules/generated/sklearn.inspection.permutation_importance.html#sklearn.inspection.permutation_importance).\n",
    "\n",
    "Extract impurity-based feature importances from `best_rf` and save the values in an array called `impurity_importances`. Then calculate permutation feature importances with `best_rf` and the test data, and save the values in an array called `permutation_importances`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here - extract impurity-based feature importances from `best_rf` and save array as `impurity_importances`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here - calculate permutation feature importances with `best_rf` and save array as `permutation_importances`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the code below to plot the feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair each feature name with its importance and sort them in descending order\n",
    "features = X_test.columns\n",
    "\n",
    "impurity_feature_importance = sorted(zip(impurity_importances, features), reverse=True)\n",
    "sorted_impurity_importances = [value[0] for value in impurity_feature_importance]\n",
    "sorted_impurity_features = [value[1] for value in impurity_feature_importance]\n",
    "\n",
    "permutation_indices = np.argsort(permutation_importances)[::-1]\n",
    "sorted_permutation_importances = permutation_importances[permutation_indices]\n",
    "sorted_permutation_features = [features[i] for i in permutation_indices]\n",
    "\n",
    "#cCreate a side-by-side plot for impurity-based and permutation importances\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# plot impurity-based feature importances\n",
    "axes[0].barh(sorted_impurity_features, sorted_impurity_importances, color='lightcoral')\n",
    "axes[0].set_title('Impurity-Based Feature Importance')\n",
    "axes[0].set_xlabel('Importance')\n",
    "axes[0].invert_yaxis()  # highest importance at the top\n",
    "\n",
    "# plot permutation feature importances\n",
    "axes[1].barh(sorted_permutation_features, sorted_permutation_importances, color='steelblue')\n",
    "axes[1].set_title('Permutation Feature Importance')\n",
    "axes[1].set_xlabel('Importance')\n",
    "axes[1].invert_yaxis()  # highest importance at the top\n",
    "\n",
    "# adjust layout \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you interpret the plots above? What can you learn about how the `RandomForestClassifier` makes predictions? Are there any differences between the impurity-based vs. permutation feature importances? Does it look like the model is discriminating based on any sensitive attribute? Or, does it look like the model is mostly influenced by legitimate attributes? Discuss with a classmate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "### Part 2: Performing a fairness assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've built our classifier, evaluated its predictive accuracy, and calculated feature importance to enhance interpretability. \n",
    "\n",
    "But remember what we're predicting and the intended use case.In practice, our model could be used to decide whether or not a human being is imprisoned. This is a high-stakes scenario. \n",
    "\n",
    "We also know, from domain knowledge, that the U.S. judicial system has a history of racial bias.  \"African Americans are more likely than white Americans to be arrested; once arrested, they are more likely to be convicted; and once convicted, and they are more likely to experience lengthy prison sentences. African-American adults are 5.9 times as likely to be incarcerated than whites\" ([The Sentencing Project](https://www.sentencingproject.org/reports/report-to-the-united-nations-on-racial-disparities-in-the-u-s-criminal-justice-system/)).\n",
    "\n",
    "With this in mind, let's perform a fairness assessment. Use the code below to create a `MetricFrame` with `fairlearn` to inspect the `selection_rate`, `true_positive_rate`, and `false_positive_rate` for `African_American` defendants vs. other defendants (i.e., our \"sensitive feature\" is `African_American`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a MetricFrame to compute metrics across groups\n",
    "metrics = MetricFrame(metrics={\n",
    "    'selection_rate': selection_rate,\n",
    "    'TPR': true_positive_rate, \n",
    "    'FPR': false_positive_rate},\n",
    "    y_true=y_test,y_pred=y_pred,\n",
    "    sensitive_features=X_test['African_American'])\n",
    "\n",
    "# print the results\n",
    "print(metrics.by_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>TASK 4</font>\n",
    "\n",
    "How do you interpret the output above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing on with our assessment, use the code below to inspect `demographic_parity_difference` across the `African_American` attribute. The demographic parity difference is defined as the difference between the largest and the smallest group-level selection rate, across all values of the sensitive feature(s). A demographic parity difference of 0 means that all groups have the same selection rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the demographic parity difference\n",
    "dpd = demographic_parity_difference(y_test, y_pred, sensitive_features=X_test[\"African_American\"])\n",
    "print(f\"Demographic Parity Difference: {dpd:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the above output suggest? How does it relate to the `MetricFrame` you printed before?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The demographic parity difference shows us the difference in `selection_rate` across groups based on our sensitive feature. The above doesn't look good. But looking at selection rates alone is just one view of unfairness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>TASK 5</font>\n",
    "\n",
    "Calculate and print the `equalized_odds_difference` below. What does it tell you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here -  compute and print the equalized odds difference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "### Part 3: Mitgating unfairness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our assessment above suggests our classifier is making unfair predictions. The classifier is more likely to label African American defendants as recidivism risks and makes more inaccurate predictions (per the TPR and FPR) for African American defendants, as compared to other defendants.\n",
    "\n",
    "There are several techniques for mitigating unfairness, such as removing correlations between your sensitive feature and other non-sensitive features in preprocessing, or imposing fairness constraints in model training/optimization. Check out the `fairlearn` documentation [here](https://fairlearn.org/v0.9/user_guide/mitigation/index.html) to learn more and see if there's a specific technique that suits your course project. \n",
    "\n",
    "For the purpose of the binary classification task in this exercise, we'll apply a postprocessing mitigation technique called *threshold optimization*. We'll keep our tuned classifier as is, rather than returning to data preprocessing or model training/optimization. Instead, what we'll do is apply the `ThreshholdOptimizer` from `fairlearn` to identify the classification threshold that satisfies our desired fairness metric.\n",
    "\n",
    "Use the code below to apply the `ThreshholdOptimizer` to identify the threshold that satisfies equality of odds, and then re-predict class labels based on that threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the equalized odds post-processor\n",
    "postprocessor = ThresholdOptimizer(\n",
    "    estimator=best_rf,\n",
    "    constraints=\"equalized_odds\",\n",
    "    predict_method='auto',\n",
    "    prefit=True\n",
    ")\n",
    "\n",
    "# fit the post-processor\n",
    "postprocessor.fit(X_test, y_test, sensitive_features=X_test['African_American'])\n",
    "\n",
    "# predict using the mitigated model\n",
    "y_pred_mitigated = postprocessor.predict(X_test, sensitive_features=X_test['African_American'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>TASK 6</font>\n",
    "\n",
    "Re-calculate and print the `equalized_odds_difference` below just like you did in Task 3, but with the mitgated predictions this time. Then re-calculate the `MetricFrame` you inspected at the start of Part 2 in this notebook, but with the mitgated predictions this time. \n",
    "\n",
    "Did the threshold optimization mitgate unfairness?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here -  re-assess the equality of odds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here - re-assess the metric frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>TASK 7</font>\n",
    "\n",
    "Finally, re-calculate a classification report with the mitigated predictions to see how your work to ensure fairness in your classifier might have affected standard measures of predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here - use the postprocessed model to make predictions on the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# your code here - print the classification report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "**That's it for this week! Next time we'll look at what it really means to say \"correlation does not imply causation\" and the practical limits of using predictive modelling for explanatory tasks.** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
