{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 08: Model Evaluation\n",
    "\n",
    "Welcome to the eighth exercise for Applied Machine Learning.\n",
    "\n",
    "Your objectives for this session are to:\n",
    "\n",
    "- implement several different machine learning models for comparison,\n",
    "- evaluate regression models with different loss functions and basic error analysis, and\n",
    "- evaluate classification models with classification reports, ROC/AUC, and gains charts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's import the libraries for today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# for making a train-tes split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# sklearn functions for evaluating regressors\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, max_error, PredictionErrorDisplay \n",
    "\n",
    "# sklearn functions for evaluating classifiers\n",
    "from sklearn.metrics import classification_report, RocCurveDisplay\n",
    "\n",
    "# dummy models for baseline\n",
    "from sklearn.dummy import DummyRegressor, DummyClassifier\n",
    "\n",
    "# regression and classification models\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the familiar libraries imported above, we'll use two new libraries in this notebook: \n",
    "* `imbalanced-learn`, for handling imbalanced data in classification tasks.\n",
    "* `scikit-plot`, for easier plotting of things like confusion matrices, calibration curves, and gains charts.\n",
    "\n",
    "If you haven't installed these packages in the past, uncomment the code in the next blocks and run the installation commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikitplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once installed, import `scikit-plot` and the `SMOTE` function from `imbalanced-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "from imblearn.over_sampling import SMOTE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "### Part 1: Regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Exercise 04 and 05 you built several regression models to predict home prices with the `HomesSoldHellerup.csv` dataset. We continue with that data here. \n",
    "\n",
    "Begin by reading the `HomesSoldHellerup.csv` file, using `pd.read_csv`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homes_df = pd.read_csv('HomesSoldHellerup.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's predict `price` with the following attributes: `m2`, `Build Year`, `Type of Sale`, `Type`.\n",
    "\n",
    "# <font color='red'>TASK 1</font>\n",
    "\n",
    "Define your feature matrix `X` and target variable `y`. (*Hint: some of the attributes are categorical so you need to make dummy variables. If you don't remember how, try looking back at Exercises 04 and 05.*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here - define X and y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following code to make a train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a baseline regressor\n",
    "\n",
    "# <font color='red'>TASK 2</font>\n",
    "\n",
    "Define a [dummy regressor](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyRegressor.html) as `dm` and fit it to the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here - define dm and fit to training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the code below to predict `y` (home price) for your test set with the dummy regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_y_pred = dm.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the code below to inspect the dummy regressor's R-squared (score) on the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Score on training set: {:.3f}\".format(dm.score(X_train, y_train)))\n",
    "print(\"Score on test set: {:.3f}\".format(dm.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you interpret these scores? What's the dummy regressor actually doing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>TASK 3</font>\n",
    "\n",
    "### Building candidate regressors\n",
    "\n",
    "Define a [linear regression model](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) as `lr` and fit it to the training data. Then predict `y` (home price) for your test set with the linear regression model. Define the predictions as an object called `lr_y_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here - define lr and fit to training data, then predict y for the test set and save predictions as lr_y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the training and test scores for the linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Score on training set: {:.3f}\".format(lr.score(X_train, y_train)))\n",
    "print(\"Score on test set: {:.3f}\".format(lr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a [k-nearest neighbor regressor model](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html) as `kn` and fit it to the training data. Then predict `y` (home price) for your test set with the KNN regressor model. Define the predictions as an object called `kn_y_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here - define kn and fit to training data, then predict y for the test set and save predictions as kn_y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the training and test scores for the KNN regressor model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Score on training set: {:.3f}\".format(kn.score(X_train, y_train)))\n",
    "print(\"Score on test set: {:.3f}\".format(kn.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a [random forest regressor model](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html?highlight=random+forest+regressor#) as `rf` and fit it to the training data. \n",
    "\n",
    "Set the following hyperparameters: `n_estimators=500` and `max_depth=5`... do you know what they mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here - define rf and fit to training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict `y` (home price) for your test set with the random forest regressor model. Define the predictions as an object called `rf_y_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here - predict y for the test set and save predictions as rf_y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the training and test scores for the random forest regressor model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Score on training set: {:.3f}\".format(rf.score(X_train, y_train)))\n",
    "print(\"Score on test set: {:.3f}\".format(rf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating your regressors\n",
    "\n",
    "Now you've fit several models: a dummy regressor as a baseline, a multiple linear regression, a KNN regressor, and a random forest regressor. Looking at the R-squared value for the training and test sets above already gives us a sense of each model's performance. But in general, what we usually care about most is not how much variance a regression model captures, as recorded with R-squared, but rather how far off predicted values are from actual, true values. So let's check out some different loss functions that we could use to communicate the predictive performace of regression models: mean absolute error (MAE), mean absolute percentage error (MAPE), mean squared error (MSE), and root mean squared error (RMSE). \n",
    "\n",
    "Use the code below to calculate MAE, MAPE, MSE, and RMSE for each regressor and then display the results in a table. Try to understand what's happening line-by-line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a function that takes actual y values (y_true) and predicted y values (y_pred) and calculates MAE, MAPE, MSE, and RMSE\n",
    "def get_metrics(y_true, y_pred):\n",
    "    \n",
    "    # MAE: the average of the absolute differences between the actual values and the predicted values\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    \n",
    "    # MAPE: the average absolute percentage difference between the actual and predicted values\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    \n",
    "    # MSE: the average of the squared differences between the actual values and the predicted values\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    \n",
    "    # RMSE: the square root of MSE\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "\n",
    "    # Max error: the average absolute percentage difference between the actual and predicted values\n",
    "    max = max_error(y_true, y_pred)\n",
    "    \n",
    "    # return all four metrics\n",
    "    return mae, mape, mse, rmse, max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# calculate the metrics for each model using our `get_metrics` function.\n",
    "dm_metrics = get_metrics(y_test, dm_y_pred)  # for dummy regressor\n",
    "lr_metrics = get_metrics(y_test, lr_y_pred)  # for linear regression\n",
    "kn_metrics = get_metrics(y_test, kn_y_pred)  # for KNN regressor\n",
    "rf_metrics = get_metrics(y_test, rf_y_pred)  # for random forest regressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create a dataframe (table) to store and display the computed metrics where each row represents a different model and each column represents a different metric\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Dummy', 'Linear Regression', 'KNN', 'Random Forest'],  # model names\n",
    "    'MAE': [dm_metrics[0], lr_metrics[0], kn_metrics[0], rf_metrics[0]],  # MAE values for each model\n",
    "    'MAPE': [dm_metrics[1], lr_metrics[1], kn_metrics[1], rf_metrics[1]],  # MAPE values for each model\n",
    "    'MSE': [dm_metrics[2], lr_metrics[2], kn_metrics[2], rf_metrics[2]],  # MSE values for each model\n",
    "    'RMSE': [dm_metrics[3], lr_metrics[3], kn_metrics[3], rf_metrics[3]], # RMSE values for each model\n",
    "    'Max': [dm_metrics[4], lr_metrics[4], kn_metrics[4], rf_metrics[4]] # Max error values for each model\n",
    "\n",
    "})\n",
    "\n",
    "# display results\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the random forest regressor is the best performing candidate model based on all metrics, but now we can clearly see that the random forest would stil be very bad in practice. For instance, looking at MAE, we see that, on average, the prices predicted by the random forest are DKK 1,535,740 off from the actual prices. Looking at MAPE, we see that, on average, the prices predicted by the random forest deviate from the actual prices by about 67.76%. If the actual price of a house is 1,000,000 DKK, the model's predicted price could be off by around 677,648 DKK on average (67.76% of 1,000,000 DKK). Looking at max error, we see that for at least one instance, the random forest predicts a price that is off by 24,743,130 DKK (!!!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics like MAE, MAPE, MSE, and RMSE are good for getting an overall sense of a regression model's predictive performance. But, as aggregate metrics, they say nothing about the types of errors a model makes. For instance, are our models over- or under-estimating house prices? This is important to consider, because in some contexts, over- and under-estimations have different real-world costs. \n",
    "\n",
    "For example, let's say that our model is intended to be used by house sellers: they should use our model and list the house on the market for the price the model predicts. If the model over-estimates the price and the seller lists the house for too high of price, the house sitting on the market longer. Maybe that's not so bad in the long run, assuming that the home is eventually sold after some negotiation. But ift he model under-estimates the price and the seller lists the house for too low of price, the seller might lose hundreds of thousands of DKK because the \"good deal\" they think they're getting is actually a bad deal based on market conditions.\n",
    "\n",
    "So, to get a sense of the kinds of errors a model is making, it's good practice to do some kind of [error analysis](https://mindfulmodeler.substack.com/p/a-simple-recipe-for-model-error-analysis). Error analysis is useful for both regression and classification models, and simply entails inspecting the instances for which a model makes erroneous predictions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>TASK 5</font>\n",
    "\n",
    "Use the `PredictionErrorDisplay` function from `sklearn` to visualize the prediction errors made by the random forest model (*Hint: you should have already defined the true y values as `y_test` and the predicted y values as `rf_y_pred`*).\n",
    "\n",
    "What can you learn from this visualization? Is the random forest model more accurate for some kinds of houses than others? Does it tend to over-estimate or under-estimate more? Are there any extraordinary prediction errors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "### Part 2: Classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Exercise 03 you used decision trees to predict whether or not a patient was likely to have a stroke with a sample from the `brain_stroke.csv` dataset. We continue with that data here, but this time using the entire dataset: `brain_stroke_full.csv`. \n",
    "\n",
    "# <font color='red'>TASK 5</font>\n",
    "\n",
    "Begin by reading the `brain_stroke_full.csv` file, and then define your feature matrix `X` and target variable `y`. \n",
    "\n",
    "Let's predict `stroke` with the following attributes: `gender`, `age`, `hypertension`, `heart_disease`, `avg_glucose_level`, `bmi`, `smoking_status`.\n",
    "\n",
    "(*Hint: some of the attributes are categorical.*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here - read in data, then define X and y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following code to make a train-test split. Note that we specify `stratify=y`, which ensures that the class label distribution in the full dataset is preserved in the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a baseline classifier\n",
    "\n",
    "# <font color='red'>TASK 6</font>\n",
    "\n",
    "Define a [dummy classifier](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html?highlight=dummy+classifier#) as `dm`, fit it to the training data, and predict `y` (the binary class label for `stroke`) for your test set. Define the predictions as an object called `dm_y_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here - define dm, fit to training data, then predict y for the test set and save predictions as dm_y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the accuracy of the dummy classifier on the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy on training set: {:.3f}\".format(dm.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(dm.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoa! The dummy classifier is super accurate. Why is that the case? What exactly is the dummy classifier doing?\n",
    "\n",
    "Let's check the balance of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y)/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like our data is quite imbalanced. Only about 5% of the instances are instances of stroke (i.e., where `y=1`). This means that even a dummy classifier that always predicts no stroke (i.e., `y=0`) displays high accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect a confusion matrix..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skplt.metrics.plot_confusion_matrix(y_test, dm_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dummy classifier is indeed just predicting `y=0` (`no stroke`) every time and getting 95% accuracy. \n",
    "\n",
    "The imbalance in the dataset means \"accuracy\" is not a good, informative indicator of classifier performance. Despite being about 95% accurate, the classifier is incorrectly classifying all cases of `stroke` as `no stroke`. That's bad! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with imbalanced data\n",
    "\n",
    "There are different approaches to dealing with an imbalanced dataset. In some cases, the best thing to do may be to [*do nothing*](https://mindfulmodeler.substack.com/p/imbalanced-data-do-nothing-should). But when the costs of different errors, false positives vs. false negatives, are not equal, then you'll likely need to do something. In our case, it seems like it'd be worse to misclassify a case of `stroke` as `no stroke` (i.e., a false negative — the patient might not get extra care and end up dying), as opposed to misclassifying a case of `no stroke` as `stroke` (i.e., a false positive — the patient might get extra care that they didn't end up needing).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Class weights**\n",
    "\n",
    "With many scikit-learn models, like [`LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), [`SVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html), [`DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html), and [`RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), there is a `class_weight` hyperparameter you can set to tell your model to weigh some class label more than others. \n",
    "\n",
    "For example, in our dataset, it's probably more important to catch positive cases where `y=1` (`stroke`) as opposed to negative cases where `y=0` (`no stroke`). So, for example, we could set `class_weight = \"balanced\"` in a logistic regression classifier to weigh the importance of positive cases as inversely proportional to class frequencies in the input data. This means that when fitting the logistic regression to the data, the optimization process will penalize errors in classifying the positive class more than it penalizes errors in classifying the negative class.\n",
    "\n",
    "But for some models, like [`KNeighborsClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html), there is no `class_weight` hyperparameter since there is no model training or fitting per se.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resampling**\n",
    "\n",
    "An alternative way to deal with imbalanced data to to apply a resampling technique. \n",
    "\n",
    "If you have a big dataset, you can *undersample* the majority class. The most straightforward way to do this is where, instead of using all the available training data, you use all instances with the minority class label and an equal amount of instances with the majority class label. For example, if we have 20,000 instances in our training set with 1,000 instances of `stroke` and 19,000 instances of `no stroke`, we'd just use 2,000 instances to train the model — the 1,000 instances of `stroke` and a random sample of 1,000 instances of `no stroke`.\n",
    "\n",
    "If you don't have much training data to begin with, undersampling is probably not a good idea. Instead, you could consider *oversampling* the minority class. The most straightforward way to do this entails randomly copying instances from the minority class until their are equal amounts of the minority and majority class. For example, if we have 20,000 instances in our training set with 1,000 instances of `stroke` and 19,000 instances of `no stroke`, we'd randomly copy instances of `stroke` until we have 19,000 and then train the model on 38,000 instances — the 19,000 oversampled instances of `stroke` and the original 19,000 instances of `no stroke`. Oversampling is useful in that it makes algorithms more sensitive to the minority class, but it can also lead to overfitting, since it introduces duplicate instances into the training set.\n",
    "\n",
    "Here, we'll implement a special kind of oversampling known as the Synthetic Minority Over-sampling Technique (SMOTE). In brief, SMOTE generates new, \"synthetic\" instances that are similiar to instances with the minority class label. In our case, it will make synthetic stroke patients (i.e., instances with `y=1`). Check out the paper introducing SMOTE [here](https://www.jair.org/index.php/jair/article/view/10302/24590), and the relevant library documentation [here](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html#). Also be aware that SMOTE isn't appropriate for every use case (see this paper [here](https://academic.oup.com/jamia/article/29/9/1525/6605096), which shows that SMOTE, undersampling, and oversampling can lead to miscalibrated probability estimates).\n",
    "\n",
    "Use the code below to implement SMOTE on the original, imbalanced stroke data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check the balance of class labels in our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_train_res)/len(y_train_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building candidate classifiers\n",
    "\n",
    "# <font color='red'>TASK 7</font>\n",
    "\n",
    "With our balanced training set, now implement a logistic regression, a KNN classifier, and a random forest classifier. \n",
    "\n",
    "For each model you should\n",
    "1. define the model,\n",
    "2. fit the model to the training data,\n",
    "3. use the model to predict class labels for the test data, and\n",
    "4. print the accuracy on the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here - implement a logistic regression here (hint: set max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here - implement a KNN classifier here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here - implement a random forest classifier here (set max depth to 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are your classifiers looking? Pretty accurate, right?\n",
    "\n",
    "But as we know, \"accuracy\" is only part of the classification story. After all, we could get an even higher accuracy with a dummy classifier on the original imbalanced data. Let's now look at measures like precision and recall to get a better sense of our classifiers' performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating your classifiers\n",
    "\n",
    "# <font color='red'>TASK 8</font>\n",
    "\n",
    "Use the [`classification_report`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) function to evaluate each of your candidate classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the logistic regression classification report here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the KNN classification report here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the random forest classification report here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the classification reports with a classmate. What can you learn from them? Of these classifiers, which would you most likely recommend in practice? \n",
    "\n",
    "Remember, we're predicting strokes. So it might be more important to avoid false negatives (incorrectly predicting that a patient *won't* have a stroke when they actually *will*) than false positives (incorrectly predicting that a patient *will* have a stroke when they actually *won't*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to metrics like precision, recall, and F1 shown in the classification reports above, a widely used tool for evaluating classifier performance is ROC/AUC. Use the code below to visualize the ROC/AUC for each candidate classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_disp = RocCurveDisplay.from_predictions(y_test, dm_y_pred, name = \"Dummy\")\n",
    "logr_disp = RocCurveDisplay.from_predictions(y_test, logr_y_pred, ax=dm_disp.ax_, name = \"Logistic reg.\")\n",
    "kn_disp = RocCurveDisplay.from_predictions(y_test, kn_y_pred, ax=dm_disp.ax_, name = \"KNN\")\n",
    "rf_disp = RocCurveDisplay.from_predictions(y_test, rf_y_pred, ax=dm_disp.ax_, name = \"Random forest\")\n",
    "\n",
    "dm_disp.figure_.suptitle(\"ROC curve comparison\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>TASK 9</font>\n",
    "\n",
    "Inspect the ROC/AUC plot above with a classmate. Based on this evaluation, which classifier seems to be the best performer? Do the AUC values suggest that and of the models are good classifiers? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr_y_probs = logr.predict_proba(X_test)\n",
    "kn_y_probs = kn.predict_proba(X_test)\n",
    "rf_y_probs = rf.predict_proba(X_test)\n",
    "\n",
    "# Create a figure with three subplots arranged in a row\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot cumulative gain for Logistic Regression in the first subplot\n",
    "skplt.metrics.plot_cumulative_gain(y_test, logr_y_probs, ax=axes[0])\n",
    "axes[0].set_title(\"Logistic Regression\")\n",
    "\n",
    "# Plot cumulative gain for K-Nearest Neighbors in the second subplot\n",
    "skplt.metrics.plot_cumulative_gain(y_test, kn_y_probs, ax=axes[1])\n",
    "axes[1].set_title(\"KNN\")\n",
    "\n",
    "# Plot cumulative gain for Random Forest in the third subplot\n",
    "skplt.metrics.plot_cumulative_gain(y_test, rf_y_probs, ax=axes[2])\n",
    "axes[2].set_title(\"Random Forest\")\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>TASK 10</font>\n",
    "\n",
    "Inspect the cumulative gains curves visualized above with a classmate. Based on this evaluation, which classifier seems to be the best performer? How does this evaluation compare with what you've already learned from the classification reports and ROC/AUC? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "**That's it for this week!** \n",
    "\n",
    "**But remember these technical evaluations of ML models don't tell us everything we need to know. Not only are there many more evaluation metrics not covered in this notebook, but there are also important social and ethical considerations to be made for a meaningful model evaluation.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
